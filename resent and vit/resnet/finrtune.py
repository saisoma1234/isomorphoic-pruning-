import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import CIFAR10
import torchvision.transforms as T
import torchvision.models as models
from tqdm import tqdm
import argparse
import torch_pruning as tp
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from collections import OrderedDict
from torchvision.models.resnet import ResNet
import wandb # <-- ADDED THIS LINE

def parse_args():
    parser = argparse.ArgumentParser(description='Fine-tune pruned/pre-trained model')
    parser.add_argument('--ckpt', required=True, type=str, help='Path to model checkpoint to load')
    parser.add_argument('--model-name', default='resnet50', type=str,
                      help='Model architecture name (e.g., resnet50)')
    parser.add_argument('--data-path', default='./data', type=str,
                      help='Dataset path')
    parser.add_argument('--train-batch-size', default=32, type=int,
                      help='Training batch size')
    parser.add_argument('--val-batch-size', default=128, type=int,
                      help='Validation batch size')
    parser.add_argument('--resize', default=224, type=int,
                      help='Image resize size for data augmentation/input')
    parser.add_argument('--epochs', default=3, type=int,
                      help='Number of epochs for fine-tuning')
    parser.add_argument('--lr', default=0.01, type=float,
                      help='Initial learning rate')
    parser.add_argument('--optimizer', default='sgd', type=str,
                      choices=['sgd', 'adamw'], help='Optimizer (sgd or adamw)')
    parser.add_argument('--momentum', default=0.9, type=float,
                      help='Momentum for SGD optimizer')
    parser.add_argument('--wd', default=5e-4, type=float,
                      help='Weight decay')
    parser.add_argument('--lr-scheduler', default='cosine', type=str,
                      choices=['cosine'], help='LR scheduler (cosine)')
    parser.add_argument('--label-smoothing', default=0.1, type=float,
                      help='Label smoothing for CrossEntropyLoss')
    parser.add_argument('--round-to', default=1, type=int,
                      help='Decimal places for metrics display')
    parser.add_argument('--is-pruned', action='store_true',
                      help='Flag for pruned models - affects how checkpoint is loaded')
    parser.add_argument('--trusted-source', action='store_true',
                      help='Allow loading untrusted pruned models (use with caution)')
    parser.add_argument('--save-as', default='./finetuned_pruned_resnet50.pth', type=str,
                      help='Path to save the fine-tuned model checkpoint')
    # --- ADDED WANDB ARGUMENTS ---
    parser.add_argument('--wandb-project', default='FineTune_Pruned_Models', type=str,
                        help='WandB project name')
    parser.add_argument('--wandb-name', default=None, type=str,
                        help='WandB run name (optional, will be autogenerated if None)')
    parser.add_argument('--wandb-mode', default='online', type=str,
                        choices=['online', 'offline', 'disabled'],
                        help='WandB run mode')
    # --- END ADDED WANDB ARGUMENTS ---
    return parser.parse_args()


def prepare_cifar10(data_path, train_batch_size, val_batch_size, resize=224):
    """Prepare CIFAR10 train and test dataloaders with specified transforms"""
    transform_train = T.Compose([
        T.RandomCrop(32, padding=4),
        T.RandomHorizontalFlip(),
        T.Resize((resize, resize)),
        T.ToTensor(),
        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    transform_test = T.Compose([
        T.Resize((resize, resize)),
        T.ToTensor(),
        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    train_set = CIFAR10(root=data_path, train=True, download=True, transform=transform_train)
    test_set = CIFAR10(root=data_path, train=False, download=True, transform=transform_test)

    train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True, num_workers=4, pin_memory=True)
    test_loader = torch.utils.data.DataLoader(test_set, batch_size=val_batch_size, shuffle=False, num_workers=4, pin_memory=True)
    return train_loader, test_loader

def evaluate_model(model, loader, device, desc='Evaluating'):
    """Evaluate model accuracy and loss"""
    model.eval()
    correct = 0
    total_loss = 0
    with torch.no_grad():
        for images, labels in tqdm(loader, desc=desc):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            total_loss += F.cross_entropy(outputs, labels, reduction='sum').item()
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
    accuracy = correct / len(loader.dataset)
    avg_loss = total_loss / len(loader.dataset)
    return accuracy, avg_loss

def safe_load_model(ckpt_path, model_name, device, is_pruned=False, trusted_source=False):
    """Safely load model with proper error handling"""
    try:
        if is_pruned:
            if trusted_source:
                model = torch.load(ckpt_path, map_location=device, weights_only=False)
            else:
                with torch.serialization.safe_globals([ResNet, nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d]):
                    model = torch.load(ckpt_path, map_location=device, weights_only=True)

            if not isinstance(model, nn.Module):
                raise ValueError("Loaded object is not a PyTorch nn.Module. Is --is-pruned correctly set?")

        else:
            model = models.__dict__[model_name](weights=None)
            if hasattr(model, 'fc') and isinstance(model.fc, nn.Linear):
                model.fc = nn.Linear(model.fc.in_features, 10)
            elif hasattr(model, 'head') and isinstance(model.head, nn.Linear):
                 model.head = nn.Linear(model.head.in_features, 10)
            else:
                print(f"Warning: Could not automatically adjust final layer for {model_name}.")

            state_dict = torch.load(ckpt_path, map_location=device)

            if all(k.startswith('module.') for k in state_dict.keys()):
                new_state_dict = OrderedDict()
                for k, v in state_dict.items():
                    name = k[7:]
                    new_state_dict[name] = v
                state_dict = new_state_dict

            model.load_state_dict(state_dict)

        return model.to(device)

    except Exception as e:
        print(f"\nError loading model from {ckpt_path}: {str(e)}")
        print("Possible solutions:")
        print("1. Ensure the --ckpt path is correct and the file exists.")
        print("2. For pruned models, try --is-pruned if you saved the full model object.")
        print("3. If using --is-pruned, try --trusted-source if you trust the file's origin.")
        print("4. Ensure the --model-name matches the checkpoint's architecture.")
        print("5. Verify the checkpoint file is not corrupted.")
        raise

def main():
    args = parse_args()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # --- WandB Initialization ---
    wandb.init(
        project=args.wandb_project,
        name=args.wandb_name,
        config=vars(args), # Logs all argparse arguments
        mode=args.wandb_mode
    )
    # --- End WandB Initialization ---

    train_loader, test_loader = prepare_cifar10(
        args.data_path, args.train_batch_size, args.val_batch_size, args.resize
    )

    try:
        print(f"\nLoading {'pruned' if args.is_pruned else 'base'} {args.model_name} from {args.ckpt} for fine-tuning...")
        model = safe_load_model(args.ckpt, args.model_name, device, args.is_pruned, args.trusted_source)
        print("Model loaded successfully.")
    except Exception:
        sys.exit(1)

    # Calculate initial MACs and Params before fine-tuning for logging purposes
    example_input = torch.randn(1, 3, args.resize, args.resize).to(device)
    initial_macs, initial_params = tp.utils.count_ops_and_params(model, example_input)
    wandb.log({
        "initial_finetune_macs_G": initial_macs / 1e9,
        "initial_finetune_params_M": initial_params / 1e6
    })

    # Define Optimizer
    if args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.wd)
    elif args.optimizer == 'adamw':
        optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)
    else:
        raise ValueError(f"Unsupported optimizer: {args.optimizer}")

    # Define LR Scheduler
    if args.lr_scheduler == 'cosine':
        scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs)
    else:
        raise ValueError(f"Unsupported LR scheduler: {args.lr_scheduler}")

    # Define Loss Function
    criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)

    print(f"\nStarting fine-tuning for {args.epochs} epochs...")
    best_accuracy = 0.0

    for epoch in range(args.epochs):
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for i, (images, labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs} Training')):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            preds = outputs.argmax(dim=1)
            total_train += labels.size(0)
            correct_train += (preds == labels).sum().item()

        train_accuracy = correct_train / total_train
        train_loss = running_loss / total_train

        scheduler.step()

        # Evaluate on validation set
        val_accuracy, val_loss = evaluate_model(model, test_loader, device, desc=f'Epoch {epoch+1}/{args.epochs} Validation')

        print(f"Epoch {epoch+1}/{args.epochs}: "
              f"Train Loss = {train_loss:.{args.round_to}f}, Train Acc = {train_accuracy:.{args.round_to}f} | "
              f"Val Loss = {val_loss:.{args.round_to}f}, Val Acc = {val_accuracy:.{args.round_to}f}")

        # --- WandB Logging per Epoch ---
        wandb.log({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "train_accuracy": train_accuracy,
            "val_loss": val_loss,
            "val_accuracy": val_accuracy,
            "learning_rate": optimizer.param_groups[0]['lr']
        })
        # --- End WandB Logging per Epoch ---

        # Save best model
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save(model, args.save_as)
            print(f"New best model saved to {args.save_as} with Val Acc = {best_accuracy:.{args.round_to}f}")
            # --- WandB Logging Best Model ---
            wandb.run.summary["best_val_accuracy"] = best_accuracy
            wandb.run.summary["best_val_loss"] = val_loss
            # Log the best model artifact if you want to store it on WandB
            # if args.save_as: # Ensure save_as is set to a path
            #     artifact = wandb.Artifact(f"{args.model_name}_finetuned_best", type="model")
            #     artifact.add_file(args.save_as)
            #     wandb.log_artifact(artifact)
            # --- End WandB Logging Best Model ---

    print("\nFine-tuning complete.")

    # Final evaluation of the saved best model
    print(f"\nLoading best fine-tuned model from {args.save_as} for final evaluation...")
    final_model = safe_load_model(args.save_as, args.model_name, device, is_pruned=True, trusted_source=args.trusted_source)
    final_accuracy, final_loss = evaluate_model(final_model, test_loader, device, desc='Final Evaluation')

    # Calculate MACs and Params of the fine-tuned model
    # Use the 'final_model' to count MACs/Params as it's the one saved as best.
    finetuned_macs, finetuned_params = tp.utils.count_ops_and_params(final_model, example_input)

    print("\n=== Final Fine-tuning Results ===")
    print(f"Best Validation Accuracy during training: {best_accuracy:.{args.round_to}f}")
    print(f"Final Evaluation Accuracy: {final_accuracy:.{args.round_to}f}")
    print(f"Final Evaluation Loss: {final_loss:.{args.round_to}f}")
    print(f"MACs: {finetuned_macs/1e9:.{args.round_to}f} G")
    print(f"Params: {finetuned_params/1e6:.{args.round_to}f} M")

    # --- WandB Final Results Logging ---
    wandb.log({
        "final_eval_accuracy": final_accuracy,
        "final_eval_loss": final_loss,
        "final_finetuned_macs_G": finetuned_macs / 1e9,
        "final_finetuned_params_M": finetuned_params / 1e6,
    })
    # Also update summary for easy access in dashboard table
    wandb.run.summary["final_eval_accuracy"] = final_accuracy
    wandb.run.summary["final_eval_loss"] = final_loss
    # --- End WandB Final Results Logging ---

    wandb.finish() # Ensure the WandB run is closed

if __name__ == '__main__':
    main()